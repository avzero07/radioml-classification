{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio ML Classification - CNN Approach\n",
    "\n",
    "Solution to the RadioML classification challenge using CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "pathToDataset = \"C:/Users/aksha/Documents/Jupyter Notebooks/radioml-classification/Datasets/Standard/RML2016.10a_dict.pkl\"\n",
    "\n",
    "# Extract the pickle file\n",
    "import pickle\n",
    "import numpy as np\n",
    "Xd = pickle.load(open(pathToDataset,'rb'),encoding=\"bytes\")\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "X = []  \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data\n",
    "\n",
    "The dataset has the size 220,000×2×128, which means that there are 220,000 entries, each consisting of an array of size 2 × 128. Each array represents the samples of about 128 µs of a received waveform sampled with approximately 106 samples/second, and it contains between 8 and 16 modulation symbols. Since the samples of the signal waveforms are complex-valued, they have been stored as real and imaginary parts, and therefore we have arrays of size 2 × 128 in the data set.\n",
    "\n",
    "That is, each row is essentially, **a + ib**.\n",
    "\n",
    "The labels of the downloaded dataset contain two parameters: the modulation technique used (one of [’8PSK’, ’AM-DSB’, ’AM-SSB’, ’BPSK’,’CPFSK’, ’GFSK’, ’PAM4’, ’QAM16’, ’QAM64’, ’QPSK’, ’WBFM’], so 11 possible modulation techniques), and the signal-to noise ratio (SNR) value (one of [−20, −18, −16, −14, −12, −10, −8, −6, −4, −2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18], so 20 possible SNR values). The SNR is a measure for the quality of the communication channel. The higher the SNR, the less “noisy” is the channel.\n",
    "\n",
    "Each item of the list is essentially like **(b'Modulation Type',SNR Value)** [Use os.fsdecode to extract Modulation Type as string] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Packages\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import tensorflow.keras.utils\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, BatchNormalization, LayerNormalization\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "The goal of this stage is to split data into Training, Validation and Testing data and then process the available data and transform (if needed) it into relevant inputs to be fed into the CNN.\n",
    "\n",
    "1. Split Dataset into Training, Validation and Testing Data : 110,000 samples for training and validation and 110,000 samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  into training and test sets of the form we can train/test on \n",
    "random.seed(777)\n",
    "np.random.seed(777)\n",
    "\n",
    "index = np.arange(0,220000)\n",
    "random.shuffle(index)\n",
    "\n",
    "trainIdx = index[0:110000]\n",
    "testIdx = index[110000:220000]\n",
    "\n",
    "trainX = X[trainIdx]\n",
    "\n",
    "\n",
    "# Create Validation Data Set\n",
    "indexVal = np.arange(0,110000)\n",
    "random.shuffle(indexVal)\n",
    "\n",
    "realTrainIdx = indexVal[0:99000] \n",
    "valIdx = indexVal[99000:110000]\n",
    "\n",
    "# Actual Training Data\n",
    "realTrainX = trainX[realTrainIdx]\n",
    "X_train = np.expand_dims(realTrainX, axis=-1) # Important\n",
    "\n",
    "# Actual Validation Data\n",
    "validX = trainX[valIdx]\n",
    "X_valid = np.expand_dims(validX, axis=-1) # Important\n",
    "\n",
    "# Actual Testing Data\n",
    "testX = X[testIdx]\n",
    "X_test = np.expand_dims(testX, axis=-1) # Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'8PSK' b'AM-DSB' b'AM-SSB' b'BPSK' b'CPFSK' b'GFSK' b'PAM4' b'QAM16'\n",
      " b'QAM64' b'QPSK' b'WBFM']\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode Labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.asarray(lbl)[:,0])\n",
    "print(lb.classes_)\n",
    "lbl_encoded=lb.transform(np.asarray(lbl)[:,0])\n",
    "ytrain=lbl_encoded[trainIdx]\n",
    "\n",
    "y_train = ytrain[realTrainIdx]\n",
    "y_valid = ytrain[valIdx]\n",
    "y_test=lbl_encoded[testIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CNN4_numFilt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bb421f862948>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minpShape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Normalization_4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropoutRate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCNN4_numFilt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCNN4_kernSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivationHidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Conv_4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minpShape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Normalization_5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropoutRate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CNN4_numFilt' is not defined"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "dropoutRate = 0.6\n",
    "\n",
    "# Structure\n",
    "inpShape = (2,128,1) # Shape of Input Data\n",
    "CNN1_numFilt = 256 # Number of Filters in CNN Layer 1\n",
    "CNN1_kernSize = (1,3) # Kernel Size of CNN Layer 1\n",
    "\n",
    "CNN2_numFilt = 256 # Number of Filters in CNN Layer 2\n",
    "CNN2_kernSize = (2,3) # Kernel Size of CNN Layer 2\n",
    "\n",
    "CNN3_numFilt = 80 # Number of Filters in CNN Layer 3\n",
    "CNN3_kernSize = (1,3) # Kernel Size of CNN Layer 3\n",
    "\n",
    "CNN4_numFilt = 80 # Number of Filters in CNN Layer 4\n",
    "CNN4_kernSize = (1,3) # Kernel Size of CNN Layer 4\n",
    "\n",
    "Dense1_numNeurons = 128 # Number of Nodes in the First Dense Layer\n",
    "numOutput = 11 # Number of Output Nodes\n",
    "\n",
    "# Weight Initialization\n",
    "weightInit = 'glorot_uniform' # Xavier Initialization\n",
    "\n",
    "# Activation Functions\n",
    "activationHidden = 'relu'\n",
    "activationOutput = 'softmax'\n",
    "\n",
    "# Loss Function\n",
    "lossFunction = 'categorical_crossentropy'\n",
    "\n",
    "# Learning Algorithm\n",
    "netOptimizer = 'adam'\n",
    "\n",
    "# Callbacks\n",
    "callbackList = [\n",
    "        tensorflow.keras.callbacks.ModelCheckpoint('Data/CNN-12/CNN-12-Weights_best.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "        tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')]\n",
    "\n",
    "# Construct Network\n",
    "model = models.Sequential()\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_1'))\n",
    "model.add(ZeroPadding2D(padding=(0,1),name='ZPad1'))\n",
    "model.add(Convolution2D(filters=CNN1_numFilt, kernel_size=CNN1_kernSize, activation=activationHidden, name='Conv_1'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_2'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(ZeroPadding2D(padding=(1,1),input_shape=inpShape,name='ZPad2'))\n",
    "model.add(Convolution2D(filters=CNN2_numFilt, kernel_size=CNN2_kernSize, activation=activationHidden, name='Conv_2'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_3'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Convolution2D(filters=CNN3_numFilt, kernel_size=CNN3_kernSize, activation=activationHidden, name='Conv_3'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_4'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Convolution2D(filters=CNN4_numFilt, kernel_size=CNN4_kernSize, activation=activationHidden, name='Conv_4'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_5'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(Dense1_numNeurons, activation=activationHidden))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_6'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Dense(numOutput, activation=activationOutput))\n",
    "model.compile(loss=lossFunction, optimizer=netOptimizer,metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 100\n",
    "batchSize = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,epochs=numEpochs,batch_size=batchSize,validation_data=(X_valid,y_valid),callbacks=callbackList,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save History\n",
    "np_loss_history = np.array(history.history[\"loss\"])\n",
    "np.save('Data/CNN-12/lossHist.npy',np_loss_history)\n",
    "\n",
    "np_accu_history = np.array(history.history[\"categorical_accuracy\"])\n",
    "np.save('Data/CNN-12/accuHist.npy',np_accu_history)\n",
    "\n",
    "np_val_loss_history = np.array(history.history[\"val_loss\"])\n",
    "np.save('Data/CNN-12/valLossHist.npy',np_val_loss_history)\n",
    "\n",
    "np_val_accu_history = np.array(history.history[\"val_categorical_accuracy\"])\n",
    "np.save('Data/CNN-12/valAccuHist.npy',np_val_accu_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Evaluate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load Best Weights\n",
    "model.load_weights('Data/CNN-12/CNN-12-Weights_best.h5')\n",
    "\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Details of History\n",
    "lHist = np.load('Data/CNN-12/lossHist.npy')\n",
    "aHist = np.load('Data/CNN-12/accuHist.npy')\n",
    "\n",
    "vLHist = np.load('Data/CNN-12/valLossHist.npy')\n",
    "vAHist = np.load('Data/CNN-12/valAccuHist.npy')\n",
    "\n",
    "# Show loss curves \n",
    "plt.figure()\n",
    "plt.title('CNN 12 - Training performance')\n",
    "plt.plot(lHist, label='Training Loss')\n",
    "plt.plot(vLHist, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0,len(lHist)+1,2),np.arange(1,len(lHist)+1,2))\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract Test Data of Specific SNR\n",
    "def extractTest(data,labels,labelsEncoded,testIndex,snr):\n",
    "    testData = data[testIndex]\n",
    "    labelArray = np.array([labels])\n",
    "    testLabels = labelArray[:,testIdx,:]\n",
    "    testLabelsEncoded = labelsEncoded[testIdx]\n",
    "    \n",
    "    idxOP = list()\n",
    "    \n",
    "    # Loop Through Label Array To Get Index of Specific SNR\n",
    "    for i in range(0,testLabels.shape[1]):\n",
    "        if testLabels[0,i,1].decode('ascii')==snr:\n",
    "            idxOP.append(i)\n",
    "    \n",
    "    # Return Subset of Test Data and Corresponding Labels\n",
    "    opTestData = np.expand_dims(testData[idxOP,:,:],axis=-1)\n",
    "    opTestLabel = testLabelsEncoded[idxOP]\n",
    "    \n",
    "    return opTestData, opTestLabel\n",
    "\n",
    "def plot_confusion_matrix(cm, titleAdd, title='CNN 11 - Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title+titleAdd)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Confusion Matrix Function\n",
    "def prepConfMat(testData,testLabel,predTestLabel,mods,title):\n",
    "    modString = list()\n",
    "    for i in range(0,len(mods)):\n",
    "        modString.append(mods[i].decode('ascii'))\n",
    "    \n",
    "    conf = np.zeros([len(mods),len(mods)])\n",
    "    confnorm = np.zeros([len(mods),len(mods)])\n",
    "    for i in range(0,testData.shape[0]):\n",
    "        j = list(testLabel[i,:]).index(1)\n",
    "        k = int(np.argmax(predTestLabel[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(mods)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    plot_confusion_matrix(confnorm, title, labels=modString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "test_Y_hat = model.predict(X_test, batch_size=1024)\n",
    "\n",
    "prepConfMat(X_test,y_test,test_Y_hat,mods,' (All SNRs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix for Specific SNR\n",
    "snr = '0'\n",
    "title = ' (SNR = '+snr+')'\n",
    "x_testSNR, y_TestSNR = extractTest(X,lbl,lbl_encoded,testIdx,snr)\n",
    "y_hat_snr = model.predict(x_testSNR, batch_size=1024)\n",
    "prepConfMat(x_testSNR,y_TestSNR,y_hat_snr,mods,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test accuracy for different SNRs\n",
    "acc = {}\n",
    "acc_array=[]\n",
    "\n",
    "snr_array=np.asarray(lbl)[:,1]\n",
    "lb_temp = preprocessing.LabelBinarizer()\n",
    "lb_temp.fit(snr_array)\n",
    "temp_array=lb_temp.classes_\n",
    "snr_label_array = []\n",
    "\n",
    "\n",
    "snr_label_array.append(temp_array[6])\n",
    "snr_label_array.append(temp_array[4])\n",
    "snr_label_array.append(temp_array[3])\n",
    "snr_label_array.append(temp_array[2])\n",
    "snr_label_array.append(temp_array[1])\n",
    "snr_label_array.append(temp_array[0])\n",
    "snr_label_array.append(temp_array[9])\n",
    "snr_label_array.append(temp_array[8])\n",
    "snr_label_array.append(temp_array[7])\n",
    "snr_label_array.append(temp_array[5])\n",
    "snr_label_array.append(temp_array[10])\n",
    "snr_label_array.append(temp_array[16])\n",
    "snr_label_array.append(temp_array[17])\n",
    "snr_label_array.append(temp_array[18])\n",
    "snr_label_array.append(temp_array[19])\n",
    "snr_label_array.append(temp_array[11])\n",
    "snr_label_array.append(temp_array[12])\n",
    "snr_label_array.append(temp_array[13])\n",
    "snr_label_array.append(temp_array[14])\n",
    "snr_label_array.append(temp_array[15])\n",
    "\n",
    "\n",
    "#print(snr_label_array)\n",
    "y_test_snr=snr_array[testIdx]\n",
    "\n",
    "\n",
    "\n",
    "for snr in snr_label_array:\n",
    "    test_X_i = X_test[np.where(y_test_snr==snr)]\n",
    "    test_Y_i = y_test[np.where(y_test_snr==snr)]\n",
    "    \n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([len(mods),len(mods)])\n",
    "    confnorm = np.zeros([len(mods),len(mods)])\n",
    "    for i in range(0,test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i,:]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(mods)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print(\"Overall Accuracy: \", cor / (cor+ncor),\"for SNR\",snr)\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "    acc_array.append(1.0*cor/(cor+ncor))\n",
    "\n",
    "print(\"Random Guess Accuracy:\",1/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show loss curves \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('CNN 12 - Accuracy vs SNRs')\n",
    "plt.plot(np.arange(-20,20,2), acc_array)\n",
    "plt.xlabel('SNR')\n",
    "plt.xticks(np.arange(-20,20,2))\n",
    "plt.ylabel('Class Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRes = np.array([acc_array])\n",
    "np.save('Data/CNN-12/accResSNR.npy',accRes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
