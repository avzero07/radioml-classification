{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECE-571M Course Project\n",
    "## Modulation Classification Using Neural Networks\n",
    "### Submission by Akshay Viswakumar (#32971665)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Acquire Data\n",
    "\n",
    "This section is just to download the RadioML2016.10a Dataset in case it is not present in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from opensig\n",
    "\n",
    "# Note: If the RML2016.10a.tar.bz2 file is in the same directory as this notebook, the file will not be downloaded again.\n",
    "\n",
    "from pathlib import Path\n",
    "dset = Path(\"RML2016.10a.tar.bz2\")\n",
    "\n",
    "# Check if the File Exists\n",
    "\n",
    "if(!dset.is_file()):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve('http://opendata.deepsig.io/datasets/2016.10/RML2016.10a.tar.bz2', 'RML2016.10a.tar.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress the RML2016.10a.tar.bz2 file into RML2016.10a.tar file\n",
    "\n",
    "# Note: If the RML2016.10a.tar file exists, then this operation is skipped.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "\n",
    "tarfile = Path(\"RML2016.10a.tar\")\n",
    "\n",
    "# Check if the Tar File Exists\n",
    "\n",
    "if(!tarfile.is_file()):\n",
    "    zipfile = bz2.BZ2File('./RML2016.10a.tar.bz2') # open the file\n",
    "    data = zipfile.read() # get the decompressed data\n",
    "    #write the .tar file\n",
    "    open('./RML2016.10a.tar', 'wb').write(data) # write a uncompressed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the .tar file to get RML2016.10a_dict.pkl\n",
    "\n",
    "# Note: If the RML2016.10a.tar file exists, then this operation is skipped.\n",
    "\n",
    "import tarfile\n",
    "\n",
    "pklFile = Path(\"RML2016.10a_dict.pkl\")\n",
    "\n",
    "# Check if the pkl File Exists\n",
    "\n",
    "if(!pklFile.si_file()):\n",
    "    my_tar = tarfile.open('./RML2016.10a.tar')\n",
    "    my_tar.extractall('./') # specify which folder to extract to\n",
    "    my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract the Pickle File and Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the pickle file\n",
    "import pickle\n",
    "import numpy as np\n",
    "Xd = pickle.load(open(pathToDataset,'rb'),encoding=\"bytes\")\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "X = []  \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Packages\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import tensorflow.keras.utils\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, BatchNormalization, LayerNormalization\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Pre-Processing\n",
    "\n",
    "Split data into a training, testing and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(777)     # To ensure that the dataset is split in a deterministic way.\n",
    "np.random.seed(777)  # To ensure that the dataset is split in a deterministic way.\n",
    "\n",
    "# This section of the code shuffles and splits the into Training, Testing and Validation Sets.\n",
    "\n",
    "index = np.arange(0,220000)\n",
    "random.shuffle(index)\n",
    "\n",
    "trainIdx = index[0:110000]\n",
    "testIdx = index[110000:220000]\n",
    "\n",
    "trainX = X[trainIdx]\n",
    "\n",
    "# Create Validation Data Set\n",
    "indexVal = np.arange(0,110000)\n",
    "random.shuffle(indexVal)\n",
    "\n",
    "realTrainIdx = indexVal[0:99000] \n",
    "valIdx = indexVal[99000:110000]\n",
    "\n",
    "# Training Data\n",
    "realTrainX = trainX[realTrainIdx]\n",
    "X_trainDNN = realTrainX # For DNN\n",
    "X_trainCNN = np.expand_dims(realTrainX, axis=-1) # For CNN\n",
    "\n",
    "# Validation Data\n",
    "validX = trainX[valIdx]\n",
    "X_validDNN = validX # For DNN\n",
    "X_validCNN = np.expand_dims(validX, axis=-1) # For CNN\n",
    "\n",
    "# Actual Testing Data\n",
    "testX = X[testIdx]\n",
    "X_testDNN = testX # For DNN\n",
    "X_testCNN = np.expand_dims(testX, axis=-1) # For CNN\n",
    "\n",
    "# This Section of the code Prepapres labels Using One-Hot Encoding\n",
    "# One Hot Encode Labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.asarray(lbl)[:,0])\n",
    "print(lb.classes_)\n",
    "lbl_encoded=lb.transform(np.asarray(lbl)[:,0])\n",
    "ytrain=lbl_encoded[trainIdx]\n",
    "\n",
    "# Labels for Training Data\n",
    "y_train = ytrain[realTrainIdx]\n",
    "\n",
    "# Labels for Validation Data\n",
    "y_valid = ytrain[valIdx]\n",
    "\n",
    "# Labels for Testing Data\n",
    "y_test=lbl_encoded[testIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A) DNN Based Solution\n",
    "\n",
    "Section 4.A) will construct the DNN Network and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
