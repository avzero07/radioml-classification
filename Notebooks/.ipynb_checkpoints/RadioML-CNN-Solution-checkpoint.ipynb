{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio ML Classification - CNN Approach\n",
    "\n",
    "Solution to the RadioML classification challenge using CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "pathToDataset = \"C:/Users/aksha/Documents/Jupyter Notebooks/radioml-classification/Datasets/Standard/RML2016.10a_dict.pkl\"\n",
    "\n",
    "# Extract the pickle file\n",
    "import pickle\n",
    "import numpy as np\n",
    "Xd = pickle.load(open(pathToDataset,'rb'),encoding=\"bytes\")\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "X = []  \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data\n",
    "\n",
    "The dataset has the size 220,000×2×128, which means that there are 220,000 entries, each consisting of an array of size 2 × 128. Each array represents the samples of about 128 µs of a received waveform sampled with approximately 106 samples/second, and it contains between 8 and 16 modulation symbols. Since the samples of the signal waveforms are complex-valued, they have been stored as real and imaginary parts, and therefore we have arrays of size 2 × 128 in the data set.\n",
    "\n",
    "That is, each row is essentially, **a + ib**.\n",
    "\n",
    "The labels of the downloaded dataset contain two parameters: the modulation technique used (one of [’8PSK’, ’AM-DSB’, ’AM-SSB’, ’BPSK’,’CPFSK’, ’GFSK’, ’PAM4’, ’QAM16’, ’QAM64’, ’QPSK’, ’WBFM’], so 11 possible modulation techniques), and the signal-to noise ratio (SNR) value (one of [−20, −18, −16, −14, −12, −10, −8, −6, −4, −2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18], so 20 possible SNR values). The SNR is a measure for the quality of the communication channel. The higher the SNR, the less “noisy” is the channel.\n",
    "\n",
    "Each item of the list is essentially like **(b'Modulation Type',SNR Value)** [Use os.fsdecode to extract Modulation Type as string] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Packages\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import tensorflow.keras.utils\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "The goal of this stage is to split data into Training, Validation and Testing data and then process the available data and transform (if needed) it into relevant inputs to be fed into the CNN.\n",
    "\n",
    "1. Split Dataset into Training, Validation and Testing Data : 110,000 samples for training and validation and 110,000 samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  into training and test sets of the form we can train/test on \n",
    "np.random.seed(777)\n",
    "\n",
    "index = np.arange(0,220000)\n",
    "random.shuffle(index)\n",
    "\n",
    "trainIdx = index[0:110000]\n",
    "testIdx = index[110000:220000]\n",
    "\n",
    "trainX = X[trainIdx]\n",
    "X_train = np.expand_dims(trainX, axis=-1) # Important\n",
    "\n",
    "testX = X[testIdx]\n",
    "X_test = np.expand_dims(testX, axis=-1) # Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'8PSK' b'AM-DSB' b'AM-SSB' b'BPSK' b'CPFSK' b'GFSK' b'PAM4' b'QAM16'\n",
      " b'QAM64' b'QPSK' b'WBFM']\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode Labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.asarray(lbl)[:,0])\n",
    "print(lb.classes_)\n",
    "lbl_encoded=lb.transform(np.asarray(lbl)[:,0])\n",
    "y_train=lbl_encoded[trainIdx]\n",
    "y_test=lbl_encoded[testIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 2, 126, 256)       1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 126, 256)       0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 1, 124, 80)        122960    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 124, 80)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2539776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                2827      \n",
      "=================================================================\n",
      "Total params: 2,666,587\n",
      "Trainable params: 2,666,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "dropoutRate = 0.6\n",
    "\n",
    "# Structure\n",
    "inpShape = (2,128,1) # Shape of Input Data\n",
    "CNN1_numFilt = 256 # Number of Filters in CNN Layer 1\n",
    "CNN1_kernSize = (1,3) # Kernel Size of CNN Layer 1\n",
    "\n",
    "CNN2_numFilt = 80 # Number of Filters in CNN Layer 2\n",
    "CNN2_kernSize = (2,3) # Kernel Size of CNN Layer 2\n",
    "\n",
    "Dense1_numNeurons = 256 # Number of Nodes in the First Dense Layer\n",
    "numOutput = 11 # Number of Output Nodes\n",
    "\n",
    "# Weight Initialization\n",
    "weightInit = 'glorot_uniform' # Xavier Initialization\n",
    "\n",
    "# Activation Functions\n",
    "activationHidden = 'relu'\n",
    "activationOutput = 'softmax'\n",
    "\n",
    "# Loss Function\n",
    "lossFunction = 'categorical_crossentropy'\n",
    "\n",
    "# Learning Algorithm\n",
    "netOptimizer = 'adam'\n",
    "\n",
    "# Construct Network\n",
    "model = models.Sequential()\n",
    "model.add(Convolution2D(filters=CNN1_numFilt, kernel_size=CNN1_kernSize, activation=activationHidden, input_shape=inpShape, name='Conv_1'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Convolution2D(filters=CNN2_numFilt, kernel_size=CNN2_kernSize, activation=activationHidden, name='Conv_2'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(Dense1_numNeurons, activation=activationHidden))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Dense(numOutput, activation=activationOutput))\n",
    "model.compile(loss=lossFunction, optimizer=netOptimizer,metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 11000 samples\n",
      "Epoch 1/10\n",
      "99000/99000 - 17s - loss: 1.5302 - categorical_accuracy: 0.4216 - val_loss: 1.3933 - val_categorical_accuracy: 0.4718\n",
      "Epoch 2/10\n",
      "99000/99000 - 17s - loss: 1.5136 - categorical_accuracy: 0.4266 - val_loss: 1.4080 - val_categorical_accuracy: 0.4673\n",
      "Epoch 3/10\n",
      "99000/99000 - 17s - loss: 1.4963 - categorical_accuracy: 0.4325 - val_loss: 1.3693 - val_categorical_accuracy: 0.4802\n",
      "Epoch 4/10\n",
      "99000/99000 - 17s - loss: 1.4871 - categorical_accuracy: 0.4364 - val_loss: 1.3746 - val_categorical_accuracy: 0.4756\n",
      "Epoch 5/10\n",
      "99000/99000 - 17s - loss: 1.4696 - categorical_accuracy: 0.4405 - val_loss: 1.3627 - val_categorical_accuracy: 0.4797\n",
      "Epoch 6/10\n",
      "99000/99000 - 17s - loss: 1.4609 - categorical_accuracy: 0.4425 - val_loss: 1.3454 - val_categorical_accuracy: 0.4891\n",
      "Epoch 7/10\n",
      "99000/99000 - 17s - loss: 1.4463 - categorical_accuracy: 0.4476 - val_loss: 1.3412 - val_categorical_accuracy: 0.4768\n",
      "Epoch 8/10\n",
      "99000/99000 - 17s - loss: 1.4415 - categorical_accuracy: 0.4501 - val_loss: 1.3398 - val_categorical_accuracy: 0.4861\n",
      "Epoch 9/10\n",
      "99000/99000 - 17s - loss: 1.4358 - categorical_accuracy: 0.4522 - val_loss: 1.3555 - val_categorical_accuracy: 0.4786\n",
      "Epoch 10/10\n",
      "99000/99000 - 17s - loss: 1.4318 - categorical_accuracy: 0.4533 - val_loss: 1.3674 - val_categorical_accuracy: 0.4714\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=10,batch_size=1024,validation_split=0.1,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Evaluate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000/110000 [==============================] - 24s 218us/sample - loss: 1.4220 - categorical_accuracy: 0.4683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.422012652501193, 0.46826363]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
