{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio ML Classification - ANN Approach\n",
    "\n",
    "Solution to the RadioML classification challenge using ANNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "pathToDataset = \"C:/Users/aksha/Documents/Jupyter Notebooks/radioml-classification/Datasets/Standard/RML2016.10a_dict.pkl\"\n",
    "\n",
    "# Extract the pickle file\n",
    "import pickle\n",
    "import numpy as np\n",
    "Xd = pickle.load(open(pathToDataset,'rb'),encoding=\"bytes\")\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "X = []  \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data\n",
    "\n",
    "The dataset has the size 220,000×2×128, which means that there are 220,000 entries, each consisting of an array of size 2 × 128. Each array represents the samples of about 128 µs of a received waveform sampled with approximately 106 samples/second, and it contains between 8 and 16 modulation symbols. Since the samples of the signal waveforms are complex-valued, they have been stored as real and imaginary parts, and therefore we have arrays of size 2 × 128 in the data set.\n",
    "\n",
    "That is, each row is essentially, **a + ib**.\n",
    "\n",
    "The labels of the downloaded dataset contain two parameters: the modulation technique used (one of [’8PSK’, ’AM-DSB’, ’AM-SSB’, ’BPSK’,’CPFSK’, ’GFSK’, ’PAM4’, ’QAM16’, ’QAM64’, ’QPSK’, ’WBFM’], so 11 possible modulation techniques), and the signal-to noise ratio (SNR) value (one of [−20, −18, −16, −14, −12, −10, −8, −6, −4, −2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18], so 20 possible SNR values). The SNR is a measure for the quality of the communication channel. The higher the SNR, the less “noisy” is the channel.\n",
    "\n",
    "Each item of the list is essentially like **(b'Modulation Type',SNR Value)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Packages\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import tensorflow.keras.utils\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, BatchNormalization, LayerNormalization\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import scipy.stats as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "The goal of this stage is to split data into Training, Validation and Testing data and then process the available data and transform (if needed) it into relevant inputs to be fed into the ANN.\n",
    "\n",
    "1. Split Dataset into Training, Validation and Testing Data : 110,000 samples for training and validation and 110,000 samples for testing.\n",
    "\n",
    "2. Write a function to process data and extract features which are fed into the ANN. (Refer ANN-1 Paper for Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  into training and test sets of the form we can train/test on \n",
    "random.seed(777)\n",
    "np.random.seed(777)\n",
    "\n",
    "index = np.arange(0,220000)\n",
    "random.shuffle(index)\n",
    "\n",
    "trainIdx = index[0:110000]\n",
    "testIdx = index[110000:220000]\n",
    "\n",
    "trainX = X[trainIdx]\n",
    "\n",
    "# Create Validation Data Set\n",
    "indexVal = np.arange(0,110000)\n",
    "random.shuffle(indexVal)\n",
    "\n",
    "realTrainIdx = indexVal[0:99000] \n",
    "valIdx = indexVal[99000:110000]\n",
    "\n",
    "# Actual Training Data\n",
    "realTrainX = trainX[realTrainIdx]\n",
    "\n",
    "# Actual Validation Data\n",
    "validX = trainX[valIdx]\n",
    "\n",
    "# Actual Testing Data\n",
    "testX = X[testIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'8PSK' b'AM-DSB' b'AM-SSB' b'BPSK' b'CPFSK' b'GFSK' b'PAM4' b'QAM16'\n",
      " b'QAM64' b'QPSK' b'WBFM']\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode Labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.asarray(lbl)[:,0])\n",
    "print(lb.classes_)\n",
    "lbl_encoded=lb.transform(np.asarray(lbl)[:,0])\n",
    "ytrain = lbl_encoded[trainIdx]\n",
    "y_train = ytrain[realTrainIdx]\n",
    "y_valid = ytrain[valIdx]\n",
    "y_test=lbl_encoded[testIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "\n",
    "# Normalize By Mean\n",
    "def normalizeMean(data):\n",
    "    meanData = np.mean(data,axis=2)\n",
    "    \n",
    "    dataReal = data[:,0,:]\n",
    "    meanReal = (np.array([meanData[:,0]])).T\n",
    "    normalReal = dataReal/meanReal\n",
    "    \n",
    "    dataImag = data[:,1,:]\n",
    "    meanImag = (np.array([meanData[:,1]])).T\n",
    "    normalImag = dataImag/meanImag\n",
    "    \n",
    "    return normalReal, normalImag\n",
    "\n",
    "# Compute Instantaneous Amplitude (or Magnitude)\n",
    "def instAmp(data):\n",
    "    return np.linalg.norm(data,axis=1)\n",
    "\n",
    "# Function for Raw Moment\n",
    "def rawMoment(data,n):\n",
    "    # Calculate the nth Raw Moment of The Data\n",
    "    dataRaised = np.power(data,n)\n",
    "    nthMoment = np.array([np.mean(dataRaised,axis=1)])\n",
    "    \n",
    "    return nthMoment.T\n",
    "\n",
    "# Function for (x+y)th Order Moment\n",
    "def highOrdMoment(data,x,y):\n",
    "    complexData = data[:,0,:]+(1j*data[:,1,:]) # Data In Complex Form\n",
    "    complexDataConj = np.conj(complexData) # Complex Conjugate\n",
    "    \n",
    "    finDat = np.power(complexData,x-y)*np.power(complexDataConj,y)\n",
    "    \n",
    "    finDatMean = np.array([np.mean(finDat,axis=1)]).T\n",
    "    \n",
    "    return finDatMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Methods\n",
    "\n",
    "test = np.array([[[1,2],[3,4]],[[5,6],[7,8]],[[9,10],[11,12]]]) # Test Data\n",
    "\n",
    "# Feature 1: Ratio of Real and Complex Power\n",
    "\n",
    "def betaRatio(data):\n",
    "    sumOfSquares = np.sum(np.square(data),axis=2)\n",
    "    beta = sumOfSquares[:,1]/sumOfSquares[:,0] # Q / I\n",
    "    beta = np.array([beta])\n",
    "    beta = beta.T\n",
    "    return beta\n",
    "\n",
    "# Feature 2: Standard Deviation of Direct Instantaneous Phase [IMP: Consider Noise Threshold]\n",
    "\n",
    "def sigmaDP(data,n):\n",
    "    # n is a flag for normalization\n",
    "    # n = 1 : Normalize\n",
    "    \n",
    "    if(n==0):\n",
    "        dataReal = data[:,0,:]\n",
    "        dataImag = data[:,1,:]\n",
    "    \n",
    "    if(n==1):\n",
    "        dataReal, dataImag = normalizeMean(data)\n",
    "    \n",
    "    # Perform I / R\n",
    "    tanArg = dataImag/dataReal\n",
    "    \n",
    "    # ArcTan | Instantaneous Phase\n",
    "    phase = np.arctan(tanArg)\n",
    "    \n",
    "    # Standard Deviation of Phase\n",
    "    sigDP = np.array([np.std(phase,axis=1)])\n",
    "    \n",
    "    return phase, sigDP.T # Also Returns Phase for Use Later\n",
    "\n",
    "# Feature 3: Standard Deviation of Absolute Value of Non Linear Component of Instantaneous Phase\n",
    "def sigmaAP(data,n):\n",
    "    # n is a flag for normalization\n",
    "    # n = 1 : Normalize\n",
    "    \n",
    "    if(n==0):\n",
    "        dataReal = data[:,0,:]\n",
    "        dataImag = data[:,1,:]\n",
    "    \n",
    "    if(n==1):\n",
    "        dataReal, dataImag = normalizeMean(data)\n",
    "    \n",
    "    # Perform I / R\n",
    "    tanArg = dataImag/dataReal\n",
    "    \n",
    "    # ArcTan | Instantaneous Phase\n",
    "    phase = np.arctan(tanArg)\n",
    "    phase = np.abs(phase)\n",
    "    \n",
    "    # Standard Deviation of Phase\n",
    "    sigAP = np.array([np.std(phase,axis=1)])\n",
    "    \n",
    "    return sigAP.T\n",
    "\n",
    "# Feature 4: Standard Deviation of Absolute Value of Normalized Instantaneous Amplitude\n",
    "\n",
    "# Absolute of Magnitude Basically\n",
    "    \n",
    "def sigmaAA(data):\n",
    "    instAmplitude = instAmp(data)\n",
    "    \n",
    "    # Sepcial Normalization\n",
    "    meanInstAmplitude = (np.array([np.mean(instAmplitude,axis=1)])).T\n",
    "    normInstAmplitude = (instAmplitude / meanInstAmplitude) - 1 # This is acn\n",
    "    \n",
    "    # Find Absolute of acn\n",
    "    normInstAmplitude = np.abs(normInstAmplitude)\n",
    "    \n",
    "    sigAA = np.array([np.std(normInstAmplitude,axis=1)])\n",
    "    \n",
    "    return sigAA.T\n",
    "\n",
    "# Feature 5: Standard Deviation of Absolute Normalized Centered Instantaneous Frequency\n",
    "\n",
    "# Less Accurate Estimate Using First Differences\n",
    "def sigmaAF(phase):\n",
    "    \n",
    "    # Compute Instantaneous Frequency\n",
    "    instFrequency = np.diff(phase) # Approximation of Derivative\n",
    "    \n",
    "    # Normalize and Center\n",
    "    meanInstFrequency = (np.array([np.mean(instFrequency,axis=1)])).T\n",
    "    normInstFrequency = (instFrequency / meanInstFrequency) - 1\n",
    "    \n",
    "    # Find Absolute\n",
    "    normInstFrequency = np.abs(normInstFrequency)\n",
    "    \n",
    "    # Find Standard Deviation\n",
    "    sigAF = np.array([np.std(normInstFrequency,axis=1)])\n",
    "    \n",
    "    return sigAF.T\n",
    "\n",
    "# Feature 6: Standard Deviation of Absolute Value of Instantaneous Amplitude (Normalized w.r.t Variance)\n",
    "def sigmaV(data):\n",
    "    instAmplitude = instAmp(data)\n",
    "    \n",
    "    # Normalize w.r.t Variance\n",
    "    varInstAmplitude = (np.array([np.var(instAmplitude,axis=1)])).T\n",
    "    normInstAmplitude = np.sqrt(instAmplitude/varInstAmplitude) - 1\n",
    "    \n",
    "    # Find Absolute\n",
    "    normInstAmplitude = np.abs(normInstAmplitude)\n",
    "    \n",
    "    # Find Standard Deviation\n",
    "    sigV = np.array([np.std(normInstAmplitude,axis=1)])\n",
    "    \n",
    "    return sigV.T\n",
    "\n",
    "# Feature 7: Mixed Order Moments [M(4,2) / M(2,1)]\n",
    "def genV20(data):\n",
    "    instAmplitude = instAmp(data)\n",
    "    \n",
    "    moment4 = rawMoment(instAmplitude,4)\n",
    "    moment2 = rawMoment(instAmplitude,2) # Typo in Paper, Squared?\n",
    "    \n",
    "    return moment4 / moment2\n",
    "\n",
    "# Feature 8: Mean of Signal Magnitude\n",
    "def meanMag(data):\n",
    "    instAmplitude = instAmp(data)\n",
    "    meanMagnitude = np.array([np.mean(instAmplitude,axis=1)])\n",
    "    \n",
    "    return meanMagnitude.T\n",
    "\n",
    "# Feature 9: Normalized Square Root of Sum of Amplitudes\n",
    "def normRootSumAmp(data):\n",
    "    instAmplitude = instAmp(data)\n",
    "    sumAmpl = np.array([np.sum(instAmplitude,axis=1)]).T\n",
    "    normRoot = np.sqrt(sumAmpl)/instAmplitude.shape[1]\n",
    "    \n",
    "    return normRoot\n",
    "\n",
    "# Feature 10: Max PSD of Normalized Centered Amplitude\n",
    "def maxPSD(data):\n",
    "    instAmplitude = instAmp(data)\n",
    "    # Compute DFT\n",
    "    ampDFT = np.fft.fft(instAmplitude,axis=1)\n",
    "    # Compute Magnitude Spectrum\n",
    "    magAmpDFT = np.abs(ampDFT)\n",
    "    # Power Spectrum\n",
    "    powDFT = np.square(magAmpDFT)\n",
    "    # Max Power\n",
    "    maxPow = np.array([np.max(powDFT,axis=1)]).T\n",
    "    \n",
    "    return maxPow / maxPow.shape[0]\n",
    "\n",
    "# Feature 11: Cumulant C20\n",
    "def getC20(data):\n",
    "    m20 = highOrdMoment(data,2,0)\n",
    "    return np.abs(m20)\n",
    "\n",
    "# Feature 12: Cumulant C21\n",
    "def getC21(data):\n",
    "    m21 = highOrdMoment(data,2,1)\n",
    "    return np.abs(m21)\n",
    "\n",
    "# Feature 13: Cumulant C40\n",
    "def getC40(data):\n",
    "    m40 = highOrdMoment(data,4,0)\n",
    "    m20 = highOrdMoment(data,2,0)\n",
    "    c40 = m40 - (3*np.square(m20))\n",
    "    \n",
    "    return np.abs(c40)\n",
    "    \n",
    "# Feature 14: Cumulant C41\n",
    "def getC41(data):\n",
    "    m41 = highOrdMoment(data,4,1)\n",
    "    m21 = highOrdMoment(data,2,1)\n",
    "    m20 = highOrdMoment(data,2,0)\n",
    "    \n",
    "    c41 = m41 - (3*m20*m21)\n",
    "    \n",
    "    return np.abs(c41)\n",
    "    \n",
    "# Feature 15: Cumulant C42\n",
    "def getC42(data):\n",
    "    m42 = highOrdMoment(data,4,2)\n",
    "    m21 = highOrdMoment(data,4,2)\n",
    "    m20 = highOrdMoment(data,2,0)\n",
    "    \n",
    "    c42 = m42 - np.square(m20) - (2*np.square(m21))\n",
    "    \n",
    "    return np.abs(c42)\n",
    "\n",
    "# Feature 16: Cumulant C63\n",
    "def getC63(data):\n",
    "    m63 = highOrdMoment(data,6,3)\n",
    "    m20 = highOrdMoment(data,2,0)\n",
    "    m21 = highOrdMoment(data,2,1)\n",
    "    m22 = highOrdMoment(data,2,2)\n",
    "    m40 = highOrdMoment(data,4,0)\n",
    "    m41 = highOrdMoment(data,4,1)\n",
    "    m42 = highOrdMoment(data,4,2)\n",
    "    \n",
    "    t1 = m63 - (9*m21*m42) + (12*np.power(m21,3))\n",
    "    #t2 = (-3*m20*m42)-(3*m22*m41)\n",
    "    #t3 = 18*m20*m21*m22\n",
    "    \n",
    "    t2 = (-6*m20*m40) + (18*np.square(m20)*m21) \n",
    "    \n",
    "    #c63 = t1+t2+t3\n",
    "    c63 = t1+t2\n",
    "    \n",
    "    return np.abs(c63)\n",
    "\n",
    "# Feature 17: Cumulant C80\n",
    "def getC80(data):\n",
    "    m80 = highOrdMoment(data,8,0)\n",
    "    m60 = highOrdMoment(data,6,0)\n",
    "    m40 = highOrdMoment(data,4,0)\n",
    "    m20 = highOrdMoment(data,2,0)\n",
    "    \n",
    "    t1 = m80 - (35*np.square(m40))\n",
    "    t2 = (-28*m60*m20) + (420*m40)\n",
    "    t3 = (-630*np.power(m20,4))\n",
    "    \n",
    "    c80 = t1+t2+t3\n",
    "    \n",
    "    return np.abs(c80)\n",
    "\n",
    "# Feature 18: Kurtosis\n",
    "def getKurtosis(data):\n",
    "    complexData = data[:,0,:]+(1j*data[:,1,:]) # Data In Complex Form\n",
    "    meanComplexData = np.array([np.mean(complexData,axis=1)]).T\n",
    "    \n",
    "    # Find fourth central moment\n",
    "    fourthPower = np.power(complexData - meanComplexData,4)\n",
    "    centralMoment4 = (np.array([np.sum(fourthPower,axis=1)]).T)/fourthPower.shape[1]\n",
    "    \n",
    "    # Variance\n",
    "    var = np.array([np.var(complexData,axis=1)]).T\n",
    "    \n",
    "    kurt = np.abs(centralMoment4)/(np.square(var)) # var already abs\n",
    "    \n",
    "    return kurt\n",
    "\n",
    "# Feature 19: Skewness\n",
    "def getSkewness(data):\n",
    "    complexData = data[:,0,:]+(1j*data[:,1,:]) # Data In Complex Form\n",
    "    meanComplexData = np.array([np.mean(complexData,axis=1)]).T\n",
    "    \n",
    "    # Find third central moment\n",
    "    thirdPower = np.power(complexData - meanComplexData,3)\n",
    "    centralMoment3 = (np.array([np.sum(thirdPower,axis=1)]).T)/thirdPower.shape[1]\n",
    "    \n",
    "    # Standard Deviation\n",
    "    std = np.array([np.std(complexData,axis=1)]).T\n",
    "    \n",
    "    skew = np.abs(centralMoment3)/(np.power(std,3)) # std already abs\n",
    "    \n",
    "    return skew\n",
    "\n",
    "# Feature 20: Peak to RMS Ratio\n",
    "def getPR(data):\n",
    "    complexData = data[:,0,:]+(1j*data[:,1,:]) # Data In Complex Form\n",
    "    absSquare = np.square(np.abs(complexData))\n",
    "    absSquareMax = np.array([np.max(absSquare,axis=1)]).T\n",
    "    \n",
    "    # Calculate RMS (without Root)\n",
    "    rms = np.array([np.mean(absSquare,axis=1)]).T # Consider Abs of Mean?\n",
    "    \n",
    "    # Calculate PR\n",
    "    PR = absSquareMax/rms\n",
    "    \n",
    "    return PR\n",
    "\n",
    "# Feature 21: Peak to Average Ratio\n",
    "def getPA(data):\n",
    "    complexData = data[:,0,:]+(1j*data[:,1,:]) # Data In Complex Form\n",
    "    absData = np.abs(complexData)\n",
    "    absMax = np.array([np.max(absData,axis=1)]).T\n",
    "    \n",
    "    # Calculate Mean\n",
    "    meanData = np.array([np.mean(absData,axis=1)]).T # Consider Abs of Mean?\n",
    "    \n",
    "    # Calculate PA\n",
    "    PA = absMax / meanData\n",
    "    \n",
    "    return PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Input vector\n",
    "\n",
    "def createIPVector(data):\n",
    "    beta = betaRatio(data)\n",
    "    sigPhase, sigDp = sigmaDP(data,1)\n",
    "    sigAp = sigmaAP(data,1)\n",
    "    sigAa = sigmaAA(data)\n",
    "    sigAF = sigmaAF(sigPhase)\n",
    "    sigV = sigmaV(data)\n",
    "    v20 = genV20(data)\n",
    "    meanMagX = meanMag(data)\n",
    "    X2 = normRootSumAmp(data)\n",
    "    gammaMax = maxPSD(data)\n",
    "    \n",
    "    cumulantC20 = getC20(data)\n",
    "    cumulantC21 = getC21(data)\n",
    "    cumulantC40 = getC40(data)\n",
    "    cumulantC41 = getC41(data)\n",
    "    cumulantC42 = getC42(data)\n",
    "    cumulantC63 = getC63(data)\n",
    "    cumulantC80 = getC80(data)\n",
    "    \n",
    "    kurtosis = getKurtosis(data)\n",
    "    skewness = getSkewness(data)\n",
    "    \n",
    "    pr = getPR(data)\n",
    "    pa = getPA(data)\n",
    "\n",
    "    # Concat\n",
    "    xtrainIP = np.concatenate((beta,sigDp,sigAp,sigAa,sigAF,sigV,v20,meanMagX,X2,gammaMax,cumulantC20,cumulantC21,cumulantC40,cumulantC41,cumulantC42,cumulantC63,cumulantC80,kurtosis,skewness,pr,pa),axis=1)\n",
    "    return xtrainIP\n",
    "\n",
    "# Generate Training, Validation and Testing Input Vectors\n",
    "xtrainIP = createIPVector(realTrainX)\n",
    "#xtrainIP = np.expand_dims(xtrainIP, axis=-1) # Important\n",
    "\n",
    "xvalidIP = createIPVector(validX)\n",
    "\n",
    "xtestIP = createIPVector(testX)\n",
    "#xtestIP = np.expand_dims(xtestIP, axis=-1) # Important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Normalization (LayerNormaliz (None, 21)                42        \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 40)                880       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 11)                121       \n",
      "=================================================================\n",
      "Total params: 2,073\n",
      "Trainable params: 2,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "dropoutRate = 0.5\n",
    "\n",
    "# Structure\n",
    "numInput = 21 # Number of Input Nodes\n",
    "numHid1 = 40 # Number of Nodes in the First Hidden Layer\n",
    "numHid2 = 20 # Number of Nodes in the Second Hidden Layer\n",
    "numHid3 = 10  # Number of Nodes in the Third Hidden Layer\n",
    "numOutput = 11 # Number of Output Nodes\n",
    "\n",
    "# Weight Initialization\n",
    "weightInit = 'glorot_uniform' # Xavier Initialization\n",
    "\n",
    "# Activation Functions\n",
    "activationHidden = 'relu'\n",
    "activationOutput = 'softmax'\n",
    "\n",
    "# Loss Function\n",
    "lossFunction = 'categorical_crossentropy'\n",
    "\n",
    "# Learning Algorithm\n",
    "#netOptimizer = SGD(learning_rate=0.001, momentum=0.9, nesterov=False)\n",
    "netOptimizer = 'adam'\n",
    "\n",
    "# Callbacks\n",
    "callbackList = [\n",
    "        tensorflow.keras.callbacks.ModelCheckpoint('Data/ANN-4/ANN-4-Weights_best.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "        tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')]\n",
    "\n",
    "# Construct Network\n",
    "model = models.Sequential()\n",
    "#model.add(BatchNormalization(axis=-1,input_shape=(11,), name='Normalization_1'))\n",
    "model.add(LayerNormalization(input_shape=(numInput,), name='Normalization'))\n",
    "#model.add(Dense(numHid1, activation=activationHidden, kernel_initializer=weightInit, name='Hidden_Layer_1'))\n",
    "model.add(Dense(numHid1, activation=activationHidden, kernel_initializer=weightInit, name='Hidden_Layer_1'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Dense(numHid2, activation=activationHidden, kernel_initializer=weightInit, name='Hidden_Layer_2'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Dense(numHid3, activation=activationHidden, kernel_initializer=weightInit, name='Hidden_Layer_3'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Dense(numOutput, activation=activationOutput, name='Output_Layer'))\n",
    "model.compile(loss=lossFunction, optimizer=netOptimizer,metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 11000 samples\n",
      "Epoch 1/100\n",
      "99000/99000 - 2s - loss: 2.5802 - categorical_accuracy: 0.0908 - val_loss: 2.3972 - val_categorical_accuracy: 0.1063\n",
      "Epoch 2/100\n",
      "99000/99000 - 0s - loss: 2.4058 - categorical_accuracy: 0.0936 - val_loss: 2.3975 - val_categorical_accuracy: 0.0995\n",
      "Epoch 3/100\n",
      "99000/99000 - 0s - loss: 2.4006 - categorical_accuracy: 0.0912 - val_loss: 2.3972 - val_categorical_accuracy: 0.1002\n",
      "Epoch 4/100\n",
      "99000/99000 - 0s - loss: 2.3990 - categorical_accuracy: 0.0925 - val_loss: 2.3970 - val_categorical_accuracy: 0.0999\n",
      "Epoch 5/100\n",
      "99000/99000 - 0s - loss: 2.3982 - categorical_accuracy: 0.0915 - val_loss: 2.3966 - val_categorical_accuracy: 0.1000\n",
      "Epoch 6/100\n",
      "99000/99000 - 0s - loss: 2.3977 - categorical_accuracy: 0.0936 - val_loss: 2.3961 - val_categorical_accuracy: 0.1006\n",
      "Epoch 7/100\n",
      "99000/99000 - 0s - loss: 2.3969 - categorical_accuracy: 0.0928 - val_loss: 2.3950 - val_categorical_accuracy: 0.1025\n",
      "Epoch 8/100\n",
      "99000/99000 - 0s - loss: 2.3960 - categorical_accuracy: 0.0946 - val_loss: 2.3933 - val_categorical_accuracy: 0.1023\n",
      "Epoch 9/100\n",
      "99000/99000 - 0s - loss: 2.3957 - categorical_accuracy: 0.0972 - val_loss: 2.3925 - val_categorical_accuracy: 0.1050\n",
      "Epoch 10/100\n",
      "99000/99000 - 0s - loss: 2.3941 - categorical_accuracy: 0.0983 - val_loss: 2.3904 - val_categorical_accuracy: 0.1047\n",
      "Epoch 11/100\n",
      "99000/99000 - 0s - loss: 2.3936 - categorical_accuracy: 0.0983 - val_loss: 2.3889 - val_categorical_accuracy: 0.1070\n",
      "Epoch 12/100\n",
      "99000/99000 - 0s - loss: 2.3915 - categorical_accuracy: 0.1001 - val_loss: 2.3864 - val_categorical_accuracy: 0.1087\n",
      "Epoch 13/100\n",
      "99000/99000 - 0s - loss: 2.3907 - categorical_accuracy: 0.0995 - val_loss: 2.3831 - val_categorical_accuracy: 0.1124\n",
      "Epoch 14/100\n",
      "99000/99000 - 0s - loss: 2.3873 - categorical_accuracy: 0.1008 - val_loss: 2.3780 - val_categorical_accuracy: 0.1161\n",
      "Epoch 15/100\n",
      "99000/99000 - 0s - loss: 2.3824 - categorical_accuracy: 0.1051 - val_loss: 2.3711 - val_categorical_accuracy: 0.1205\n",
      "Epoch 16/100\n",
      "99000/99000 - 0s - loss: 2.3776 - categorical_accuracy: 0.1066 - val_loss: 2.3620 - val_categorical_accuracy: 0.1213\n",
      "Epoch 17/100\n",
      "99000/99000 - 0s - loss: 2.3707 - categorical_accuracy: 0.1096 - val_loss: 2.3519 - val_categorical_accuracy: 0.1270\n",
      "Epoch 18/100\n",
      "99000/99000 - 0s - loss: 2.3655 - categorical_accuracy: 0.1102 - val_loss: 2.3389 - val_categorical_accuracy: 0.1277\n",
      "Epoch 19/100\n",
      "99000/99000 - 0s - loss: 2.3593 - categorical_accuracy: 0.1129 - val_loss: 2.3275 - val_categorical_accuracy: 0.1306\n",
      "Epoch 20/100\n",
      "99000/99000 - 0s - loss: 2.3501 - categorical_accuracy: 0.1138 - val_loss: 2.3217 - val_categorical_accuracy: 0.1254\n",
      "Epoch 21/100\n",
      "99000/99000 - 0s - loss: 2.3363 - categorical_accuracy: 0.1169 - val_loss: 2.3034 - val_categorical_accuracy: 0.1312\n",
      "Epoch 22/100\n",
      "99000/99000 - 0s - loss: 2.3253 - categorical_accuracy: 0.1190 - val_loss: 2.2945 - val_categorical_accuracy: 0.1295\n",
      "Epoch 23/100\n",
      "99000/99000 - 0s - loss: 2.3200 - categorical_accuracy: 0.1204 - val_loss: 2.2814 - val_categorical_accuracy: 0.1362\n",
      "Epoch 24/100\n",
      "99000/99000 - 0s - loss: 2.3119 - categorical_accuracy: 0.1230 - val_loss: 2.2760 - val_categorical_accuracy: 0.1425\n",
      "Epoch 25/100\n",
      "99000/99000 - 0s - loss: 2.3076 - categorical_accuracy: 0.1235 - val_loss: 2.2689 - val_categorical_accuracy: 0.1496\n",
      "Epoch 26/100\n",
      "99000/99000 - 0s - loss: 2.3033 - categorical_accuracy: 0.1246 - val_loss: 2.2648 - val_categorical_accuracy: 0.1441\n",
      "Epoch 27/100\n",
      "99000/99000 - 0s - loss: 2.2992 - categorical_accuracy: 0.1253 - val_loss: 2.2640 - val_categorical_accuracy: 0.1435\n",
      "Epoch 28/100\n",
      "99000/99000 - 0s - loss: 2.2943 - categorical_accuracy: 0.1254 - val_loss: 2.2668 - val_categorical_accuracy: 0.1325\n",
      "Epoch 29/100\n",
      "99000/99000 - 0s - loss: 2.2916 - categorical_accuracy: 0.1271 - val_loss: 2.2548 - val_categorical_accuracy: 0.1394\n",
      "Epoch 30/100\n",
      "99000/99000 - 0s - loss: 2.2882 - categorical_accuracy: 0.1274 - val_loss: 2.2610 - val_categorical_accuracy: 0.1339\n",
      "Epoch 31/100\n",
      "99000/99000 - 0s - loss: 2.2925 - categorical_accuracy: 0.1285 - val_loss: 2.2570 - val_categorical_accuracy: 0.1401\n",
      "Epoch 32/100\n",
      "99000/99000 - 0s - loss: 2.2853 - categorical_accuracy: 0.1337 - val_loss: 2.2574 - val_categorical_accuracy: 0.1353\n",
      "Epoch 33/100\n",
      "99000/99000 - 0s - loss: 2.2849 - categorical_accuracy: 0.1361 - val_loss: 2.2496 - val_categorical_accuracy: 0.1430\n",
      "Epoch 34/100\n",
      "99000/99000 - 0s - loss: 2.2832 - categorical_accuracy: 0.1356 - val_loss: 2.2494 - val_categorical_accuracy: 0.1432\n",
      "Epoch 35/100\n",
      "99000/99000 - 0s - loss: 2.2814 - categorical_accuracy: 0.1341 - val_loss: 2.2439 - val_categorical_accuracy: 0.1489\n",
      "Epoch 36/100\n",
      "99000/99000 - 0s - loss: 2.2788 - categorical_accuracy: 0.1389 - val_loss: 2.2460 - val_categorical_accuracy: 0.1385\n",
      "Epoch 37/100\n",
      "99000/99000 - 0s - loss: 2.2756 - categorical_accuracy: 0.1380 - val_loss: 2.2455 - val_categorical_accuracy: 0.1472\n",
      "Epoch 38/100\n",
      "99000/99000 - 0s - loss: 2.2738 - categorical_accuracy: 0.1357 - val_loss: 2.2432 - val_categorical_accuracy: 0.1488\n",
      "Epoch 39/100\n",
      "99000/99000 - 0s - loss: 2.2716 - categorical_accuracy: 0.1382 - val_loss: 2.2345 - val_categorical_accuracy: 0.1589\n",
      "Epoch 40/100\n",
      "99000/99000 - 0s - loss: 2.2692 - categorical_accuracy: 0.1397 - val_loss: 2.2399 - val_categorical_accuracy: 0.1534\n",
      "Epoch 41/100\n",
      "99000/99000 - 0s - loss: 2.2684 - categorical_accuracy: 0.1405 - val_loss: 2.2358 - val_categorical_accuracy: 0.1553\n",
      "Epoch 42/100\n",
      "99000/99000 - 0s - loss: 2.2661 - categorical_accuracy: 0.1425 - val_loss: 2.2283 - val_categorical_accuracy: 0.1737\n",
      "Epoch 43/100\n",
      "99000/99000 - 0s - loss: 2.2642 - categorical_accuracy: 0.1455 - val_loss: 2.2244 - val_categorical_accuracy: 0.1717\n",
      "Epoch 44/100\n",
      "99000/99000 - 0s - loss: 2.2624 - categorical_accuracy: 0.1458 - val_loss: 2.2181 - val_categorical_accuracy: 0.1857\n",
      "Epoch 45/100\n",
      "99000/99000 - 0s - loss: 2.2610 - categorical_accuracy: 0.1450 - val_loss: 2.2122 - val_categorical_accuracy: 0.1885\n",
      "Epoch 46/100\n",
      "99000/99000 - 0s - loss: 2.2557 - categorical_accuracy: 0.1479 - val_loss: 2.2156 - val_categorical_accuracy: 0.1815\n",
      "Epoch 47/100\n",
      "99000/99000 - 0s - loss: 2.2564 - categorical_accuracy: 0.1482 - val_loss: 2.2086 - val_categorical_accuracy: 0.1879\n",
      "Epoch 48/100\n",
      "99000/99000 - 0s - loss: 2.2480 - categorical_accuracy: 0.1513 - val_loss: 2.1952 - val_categorical_accuracy: 0.1918\n",
      "Epoch 49/100\n",
      "99000/99000 - 0s - loss: 2.2438 - categorical_accuracy: 0.1555 - val_loss: 2.1916 - val_categorical_accuracy: 0.1895\n",
      "Epoch 50/100\n",
      "99000/99000 - 0s - loss: 2.2417 - categorical_accuracy: 0.1578 - val_loss: 2.1821 - val_categorical_accuracy: 0.1999\n",
      "Epoch 51/100\n",
      "99000/99000 - 0s - loss: 2.2400 - categorical_accuracy: 0.1578 - val_loss: 2.1781 - val_categorical_accuracy: 0.1951\n",
      "Epoch 52/100\n",
      "99000/99000 - 0s - loss: 2.2347 - categorical_accuracy: 0.1608 - val_loss: 2.1710 - val_categorical_accuracy: 0.1960\n",
      "Epoch 53/100\n",
      "99000/99000 - 0s - loss: 2.2348 - categorical_accuracy: 0.1637 - val_loss: 2.1699 - val_categorical_accuracy: 0.1993\n",
      "Epoch 54/100\n",
      "99000/99000 - 0s - loss: 2.2293 - categorical_accuracy: 0.1647 - val_loss: 2.1743 - val_categorical_accuracy: 0.1966\n",
      "Epoch 55/100\n",
      "99000/99000 - 0s - loss: 2.2234 - categorical_accuracy: 0.1673 - val_loss: 2.1583 - val_categorical_accuracy: 0.2101\n",
      "Epoch 56/100\n",
      "99000/99000 - 0s - loss: 2.2266 - categorical_accuracy: 0.1661 - val_loss: 2.1599 - val_categorical_accuracy: 0.2051\n",
      "Epoch 57/100\n",
      "99000/99000 - 0s - loss: 2.2229 - categorical_accuracy: 0.1678 - val_loss: 2.1595 - val_categorical_accuracy: 0.2005\n",
      "Epoch 58/100\n",
      "99000/99000 - 0s - loss: 2.2214 - categorical_accuracy: 0.1685 - val_loss: 2.1541 - val_categorical_accuracy: 0.2101\n",
      "Epoch 59/100\n",
      "99000/99000 - 0s - loss: 2.2181 - categorical_accuracy: 0.1725 - val_loss: 2.1532 - val_categorical_accuracy: 0.2035\n",
      "Epoch 60/100\n",
      "99000/99000 - 0s - loss: 2.2192 - categorical_accuracy: 0.1718 - val_loss: 2.1502 - val_categorical_accuracy: 0.2086\n",
      "Epoch 61/100\n",
      "99000/99000 - 0s - loss: 2.2203 - categorical_accuracy: 0.1694 - val_loss: 2.1485 - val_categorical_accuracy: 0.2152\n",
      "Epoch 62/100\n",
      "99000/99000 - 0s - loss: 2.2164 - categorical_accuracy: 0.1735 - val_loss: 2.1447 - val_categorical_accuracy: 0.2037\n",
      "Epoch 63/100\n",
      "99000/99000 - 0s - loss: 2.2143 - categorical_accuracy: 0.1747 - val_loss: 2.1580 - val_categorical_accuracy: 0.2005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "99000/99000 - 0s - loss: 2.2140 - categorical_accuracy: 0.1742 - val_loss: 2.1511 - val_categorical_accuracy: 0.2015\n",
      "Epoch 65/100\n",
      "99000/99000 - 0s - loss: 2.2138 - categorical_accuracy: 0.1758 - val_loss: 2.1471 - val_categorical_accuracy: 0.2130\n",
      "Epoch 66/100\n",
      "99000/99000 - 0s - loss: 2.2157 - categorical_accuracy: 0.1740 - val_loss: 2.1435 - val_categorical_accuracy: 0.2267\n",
      "Epoch 67/100\n",
      "99000/99000 - 0s - loss: 2.2116 - categorical_accuracy: 0.1772 - val_loss: 2.1442 - val_categorical_accuracy: 0.2133\n",
      "Epoch 68/100\n",
      "99000/99000 - 0s - loss: 2.2086 - categorical_accuracy: 0.1774 - val_loss: 2.1471 - val_categorical_accuracy: 0.2139\n",
      "Epoch 69/100\n",
      "99000/99000 - 0s - loss: 2.2136 - categorical_accuracy: 0.1762 - val_loss: 2.1409 - val_categorical_accuracy: 0.2198\n",
      "Epoch 70/100\n",
      "99000/99000 - 0s - loss: 2.2070 - categorical_accuracy: 0.1793 - val_loss: 2.1426 - val_categorical_accuracy: 0.2119\n",
      "Epoch 71/100\n",
      "99000/99000 - 0s - loss: 2.2068 - categorical_accuracy: 0.1799 - val_loss: 2.1316 - val_categorical_accuracy: 0.2311\n",
      "Epoch 72/100\n",
      "99000/99000 - 0s - loss: 2.2043 - categorical_accuracy: 0.1814 - val_loss: 2.1300 - val_categorical_accuracy: 0.2292\n",
      "Epoch 73/100\n",
      "99000/99000 - 0s - loss: 2.2086 - categorical_accuracy: 0.1806 - val_loss: 2.1418 - val_categorical_accuracy: 0.2106\n",
      "Epoch 74/100\n",
      "99000/99000 - 0s - loss: 2.2037 - categorical_accuracy: 0.1810 - val_loss: 2.1294 - val_categorical_accuracy: 0.2222\n",
      "Epoch 75/100\n",
      "99000/99000 - 0s - loss: 2.2020 - categorical_accuracy: 0.1810 - val_loss: 2.1354 - val_categorical_accuracy: 0.2169\n",
      "Epoch 76/100\n",
      "99000/99000 - 0s - loss: 2.2027 - categorical_accuracy: 0.1824 - val_loss: 2.1302 - val_categorical_accuracy: 0.2365\n",
      "Epoch 77/100\n",
      "99000/99000 - 0s - loss: 2.2052 - categorical_accuracy: 0.1807 - val_loss: 2.1364 - val_categorical_accuracy: 0.2200\n",
      "Epoch 78/100\n",
      "99000/99000 - 0s - loss: 2.2032 - categorical_accuracy: 0.1835 - val_loss: 2.1289 - val_categorical_accuracy: 0.2248\n",
      "Epoch 79/100\n",
      "99000/99000 - 0s - loss: 2.2000 - categorical_accuracy: 0.1850 - val_loss: 2.1320 - val_categorical_accuracy: 0.2285\n",
      "Epoch 80/100\n",
      "99000/99000 - 0s - loss: 2.2042 - categorical_accuracy: 0.1809 - val_loss: 2.1344 - val_categorical_accuracy: 0.2177\n",
      "Epoch 81/100\n",
      "99000/99000 - 0s - loss: 2.1996 - categorical_accuracy: 0.1834 - val_loss: 2.1259 - val_categorical_accuracy: 0.2241\n",
      "Epoch 82/100\n",
      "99000/99000 - 0s - loss: 2.1998 - categorical_accuracy: 0.1862 - val_loss: 2.1256 - val_categorical_accuracy: 0.2299\n",
      "Epoch 83/100\n",
      "99000/99000 - 0s - loss: 2.1967 - categorical_accuracy: 0.1860 - val_loss: 2.1203 - val_categorical_accuracy: 0.2315\n",
      "Epoch 84/100\n",
      "99000/99000 - 0s - loss: 2.1948 - categorical_accuracy: 0.1865 - val_loss: 2.1338 - val_categorical_accuracy: 0.2181\n",
      "Epoch 85/100\n",
      "99000/99000 - 0s - loss: 2.1972 - categorical_accuracy: 0.1831 - val_loss: 2.1299 - val_categorical_accuracy: 0.2265\n",
      "Epoch 86/100\n",
      "99000/99000 - 0s - loss: 2.1937 - categorical_accuracy: 0.1884 - val_loss: 2.1159 - val_categorical_accuracy: 0.2366\n",
      "Epoch 87/100\n",
      "99000/99000 - 0s - loss: 2.1943 - categorical_accuracy: 0.1894 - val_loss: 2.1246 - val_categorical_accuracy: 0.2358\n",
      "Epoch 88/100\n",
      "99000/99000 - 0s - loss: 2.1929 - categorical_accuracy: 0.1875 - val_loss: 2.1176 - val_categorical_accuracy: 0.2345\n",
      "Epoch 89/100\n",
      "99000/99000 - 0s - loss: 2.1940 - categorical_accuracy: 0.1874 - val_loss: 2.1188 - val_categorical_accuracy: 0.2405\n",
      "Epoch 90/100\n",
      "99000/99000 - 0s - loss: 2.1927 - categorical_accuracy: 0.1895 - val_loss: 2.1206 - val_categorical_accuracy: 0.2278\n",
      "Epoch 91/100\n",
      "99000/99000 - 0s - loss: 2.1932 - categorical_accuracy: 0.1907 - val_loss: 2.1297 - val_categorical_accuracy: 0.2169\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(xtrainIP, y_train, batch_size=1024, epochs=100, verbose=2, validation_data=(xvalidIP,y_valid),callbacks=callbackList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save History\n",
    "np_loss_history = np.array(history.history[\"loss\"])\n",
    "np.save('Data/ANN-4/lossHist.npy',np_loss_history)\n",
    "\n",
    "np_accu_history = np.array(history.history[\"categorical_accuracy\"])\n",
    "np.save('Data/ANN-4/accuHist.npy',np_accu_history)\n",
    "\n",
    "np_val_loss_history = np.array(history.history[\"val_loss\"])\n",
    "np.save('Data/ANN-4/valLossHist.npy',np_val_loss_history)\n",
    "\n",
    "np_val_accu_history = np.array(history.history[\"val_categorical_accuracy\"])\n",
    "np.save('Data/ANN-4/valAccuHist.npy',np_val_accu_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Evaluate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000/110000 [==============================] - 6s 56us/sample - loss: 2.1181 - categorical_accuracy: 0.2345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1181239354913886, 0.23447272]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-load Best Weights\n",
    "model.load_weights('Data/ANN-4/ANN-4-Weights_best.h5')\n",
    "\n",
    "model.evaluate(xtestIP,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualaize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Details of History\n",
    "lHist = np.load('Data/ANN-4/lossHist.npy')\n",
    "aHist = np.load('Data/ANN-4/accuHist.npy')\n",
    "\n",
    "vLHist = np.load('Data/ANN-4/valLossHist.npy')\n",
    "vAHist = np.load('Data/ANN-4/valAccuHist.npy')\n",
    "\n",
    "# Show loss curves \n",
    "plt.figure()\n",
    "plt.title('ANN 4 - Training performance')\n",
    "plt.plot(lHist, label='Training Loss')\n",
    "plt.plot(vLHist, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0,len(lHist)+1,2),np.arange(1,len(lHist)+1,2))\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract Test Data of Specific SNR\n",
    "def extractTest(data,labels,labelsEncoded,testIndex,snr):\n",
    "    testData = data[testIndex]\n",
    "    labelArray = np.array([labels])\n",
    "    testLabels = labelArray[:,testIdx,:]\n",
    "    testLabelsEncoded = labelsEncoded[testIdx]\n",
    "    \n",
    "    idxOP = list()\n",
    "    \n",
    "    # Loop Through Label Array To Get Index of Specific SNR\n",
    "    for i in range(0,testLabels.shape[1]):\n",
    "        if testLabels[0,i,1].decode('ascii')==snr:\n",
    "            idxOP.append(i)\n",
    "    \n",
    "    # Return Subset of Test Data and Corresponding Labels\n",
    "    opTestData = testData[idxOP,:,:]\n",
    "    opTestLabel = testLabelsEncoded[idxOP]\n",
    "    \n",
    "    return opTestData, opTestLabel\n",
    "\n",
    "def plot_confusion_matrix(cm, titleAdd, title='ANN 4 - Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title+titleAdd)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Confusion Matrix Function\n",
    "def prepConfMat(testData,testLabel,predTestLabel,mods,title):\n",
    "    modString = list()\n",
    "    for i in range(0,len(mods)):\n",
    "        modString.append(mods[i].decode('ascii'))\n",
    "    \n",
    "    conf = np.zeros([len(mods),len(mods)])\n",
    "    confnorm = np.zeros([len(mods),len(mods)])\n",
    "    for i in range(0,testData.shape[0]):\n",
    "        j = list(testLabel[i,:]).index(1)\n",
    "        k = int(np.argmax(predTestLabel[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(mods)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    plot_confusion_matrix(confnorm, title, labels=modString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "test_Y_hat = model.predict(xtestIP, batch_size=1024)\n",
    "\n",
    "prepConfMat(xtestIP,y_test,test_Y_hat,mods,' (All SNRs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix for Specific SNR\n",
    "snr = '18'\n",
    "title = ' (SNR = '+snr+')'\n",
    "x_testSNR, y_TestSNR = extractTest(X,lbl,lbl_encoded,testIdx,snr)\n",
    "\n",
    "xtestSNRFeat = createIPVector(x_testSNR)\n",
    "\n",
    "y_hat_snr = model.predict(xtestSNRFeat, batch_size=1024)\n",
    "prepConfMat(xtestSNRFeat,y_TestSNR,y_hat_snr,mods,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test accuracy for different SNRs\n",
    "acc = {}\n",
    "acc_array=[]\n",
    "\n",
    "snr_array=np.asarray(lbl)[:,1]\n",
    "lb_temp = preprocessing.LabelBinarizer()\n",
    "lb_temp.fit(snr_array)\n",
    "temp_array=lb_temp.classes_\n",
    "snr_label_array = []\n",
    "\n",
    "\n",
    "snr_label_array.append(temp_array[6])\n",
    "snr_label_array.append(temp_array[4])\n",
    "snr_label_array.append(temp_array[3])\n",
    "snr_label_array.append(temp_array[2])\n",
    "snr_label_array.append(temp_array[1])\n",
    "snr_label_array.append(temp_array[0])\n",
    "snr_label_array.append(temp_array[9])\n",
    "snr_label_array.append(temp_array[8])\n",
    "snr_label_array.append(temp_array[7])\n",
    "snr_label_array.append(temp_array[5])\n",
    "snr_label_array.append(temp_array[10])\n",
    "snr_label_array.append(temp_array[16])\n",
    "snr_label_array.append(temp_array[17])\n",
    "snr_label_array.append(temp_array[18])\n",
    "snr_label_array.append(temp_array[19])\n",
    "snr_label_array.append(temp_array[11])\n",
    "snr_label_array.append(temp_array[12])\n",
    "snr_label_array.append(temp_array[13])\n",
    "snr_label_array.append(temp_array[14])\n",
    "snr_label_array.append(temp_array[15])\n",
    "\n",
    "\n",
    "#print(snr_label_array)\n",
    "y_test_snr=snr_array[testIdx]\n",
    "\n",
    "\n",
    "\n",
    "for snr in snr_label_array:\n",
    "    test_X_i_temp = testX[np.where(y_test_snr==snr)]\n",
    "    test_X_i = createIPVector(test_X_i_temp)\n",
    "    test_Y_i = y_test[np.where(y_test_snr==snr)]\n",
    "    \n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([len(mods),len(mods)])\n",
    "    confnorm = np.zeros([len(mods),len(mods)])\n",
    "    for i in range(0,test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i,:]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(mods)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print(\"Overall Accuracy: \", cor / (cor+ncor),\"for SNR\",snr)\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "    acc_array.append(1.0*cor/(cor+ncor))\n",
    "\n",
    "print(\"Random Guess Accuracy:\",1/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show loss curves \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('ANN 4 - Accuracy vs SNRs')\n",
    "plt.plot(np.arange(-20,20,2), acc_array)\n",
    "plt.xlabel('SNR')\n",
    "plt.xticks(np.arange(-20,20,2))\n",
    "plt.ylabel('Class Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRes = np.array([acc_array])\n",
    "np.save('Data/ANN-4/accResSNR.npy',accRes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
