{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio ML Classification - CNN Approach\n",
    "\n",
    "Solution to the RadioML classification challenge using CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "pathToDataset = \"C:/Users/aksha/Documents/Jupyter Notebooks/radioml-classification/Datasets/Standard/RML2016.10a_dict.pkl\"\n",
    "\n",
    "# Extract the pickle file\n",
    "import pickle\n",
    "import numpy as np\n",
    "Xd = pickle.load(open(pathToDataset,'rb'),encoding=\"bytes\")\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "X = []  \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data\n",
    "\n",
    "The dataset has the size 220,000×2×128, which means that there are 220,000 entries, each consisting of an array of size 2 × 128. Each array represents the samples of about 128 µs of a received waveform sampled with approximately 106 samples/second, and it contains between 8 and 16 modulation symbols. Since the samples of the signal waveforms are complex-valued, they have been stored as real and imaginary parts, and therefore we have arrays of size 2 × 128 in the data set.\n",
    "\n",
    "That is, each row is essentially, **a + ib**.\n",
    "\n",
    "The labels of the downloaded dataset contain two parameters: the modulation technique used (one of [’8PSK’, ’AM-DSB’, ’AM-SSB’, ’BPSK’,’CPFSK’, ’GFSK’, ’PAM4’, ’QAM16’, ’QAM64’, ’QPSK’, ’WBFM’], so 11 possible modulation techniques), and the signal-to noise ratio (SNR) value (one of [−20, −18, −16, −14, −12, −10, −8, −6, −4, −2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18], so 20 possible SNR values). The SNR is a measure for the quality of the communication channel. The higher the SNR, the less “noisy” is the channel.\n",
    "\n",
    "Each item of the list is essentially like **(b'Modulation Type',SNR Value)** [Use os.fsdecode to extract Modulation Type as string] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Packages\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import tensorflow.keras.utils\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, BatchNormalization, LayerNormalization\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "The goal of this stage is to split data into Training, Validation and Testing data and then process the available data and transform (if needed) it into relevant inputs to be fed into the CNN.\n",
    "\n",
    "1. Split Dataset into Training, Validation and Testing Data : 110,000 samples for training and validation and 110,000 samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  into training and test sets of the form we can train/test on \n",
    "random.seed(777)\n",
    "np.random.seed(777)\n",
    "\n",
    "index = np.arange(0,220000)\n",
    "random.shuffle(index)\n",
    "\n",
    "trainIdx = index[0:110000]\n",
    "testIdx = index[110000:220000]\n",
    "\n",
    "trainX = X[trainIdx]\n",
    "\n",
    "\n",
    "# Create Validation Data Set\n",
    "indexVal = np.arange(0,110000)\n",
    "random.shuffle(indexVal)\n",
    "\n",
    "realTrainIdx = indexVal[0:99000] \n",
    "valIdx = indexVal[99000:110000]\n",
    "\n",
    "# Actual Training Data\n",
    "realTrainX = trainX[realTrainIdx]\n",
    "X_train = np.expand_dims(realTrainX, axis=-1) # Important\n",
    "\n",
    "# Actual Validation Data\n",
    "validX = trainX[valIdx]\n",
    "X_valid = np.expand_dims(validX, axis=-1) # Important\n",
    "\n",
    "# Actual Testing Data\n",
    "testX = X[testIdx]\n",
    "X_test = np.expand_dims(testX, axis=-1) # Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'8PSK' b'AM-DSB' b'AM-SSB' b'BPSK' b'CPFSK' b'GFSK' b'PAM4' b'QAM16'\n",
      " b'QAM64' b'QPSK' b'WBFM']\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode Labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.asarray(lbl)[:,0])\n",
    "print(lb.classes_)\n",
    "lbl_encoded=lb.transform(np.asarray(lbl)[:,0])\n",
    "ytrain=lbl_encoded[trainIdx]\n",
    "\n",
    "y_train = ytrain[realTrainIdx]\n",
    "y_valid = ytrain[valIdx]\n",
    "y_test=lbl_encoded[testIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Normalization_1 (BatchNormal (None, 2, 128, 1)         4         \n",
      "_________________________________________________________________\n",
      "ZPad1 (ZeroPadding2D)        (None, 2, 130, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv_1 (Conv2D)              (None, 2, 128, 256)       1024      \n",
      "_________________________________________________________________\n",
      "Normalization_2 (BatchNormal (None, 2, 128, 256)       1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 128, 256)       0         \n",
      "_________________________________________________________________\n",
      "ZPad2 (ZeroPadding2D)        (None, 4, 130, 256)       0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 3, 128, 256)       393472    \n",
      "_________________________________________________________________\n",
      "Normalization_3 (BatchNormal (None, 3, 128, 256)       1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 128, 256)       0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv2D)              (None, 3, 126, 80)        61520     \n",
      "_________________________________________________________________\n",
      "Normalization_4 (BatchNormal (None, 3, 126, 80)        320       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 126, 80)        0         \n",
      "_________________________________________________________________\n",
      "Conv_4 (Conv2D)              (None, 3, 124, 80)        19280     \n",
      "_________________________________________________________________\n",
      "Normalization_5 (BatchNormal (None, 3, 124, 80)        320       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 124, 80)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 29760)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               3809408   \n",
      "_________________________________________________________________\n",
      "Normalization_6 (BatchNormal (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 4,289,327\n",
      "Trainable params: 4,287,725\n",
      "Non-trainable params: 1,602\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "dropoutRate = 0.6\n",
    "\n",
    "# Structure\n",
    "inpShape = (2,128,1) # Shape of Input Data\n",
    "CNN1_numFilt = 256 # Number of Filters in CNN Layer 1\n",
    "CNN1_kernSize = (1,3) # Kernel Size of CNN Layer 1\n",
    "\n",
    "CNN2_numFilt = 256 # Number of Filters in CNN Layer 2\n",
    "CNN2_kernSize = (2,3) # Kernel Size of CNN Layer 2\n",
    "\n",
    "CNN3_numFilt = 80 # Number of Filters in CNN Layer 3\n",
    "CNN3_kernSize = (1,3) # Kernel Size of CNN Layer 3\n",
    "\n",
    "CNN4_numFilt = 80 # Number of Filters in CNN Layer 4\n",
    "CNN4_kernSize = (1,3) # Kernel Size of CNN Layer 4\n",
    "\n",
    "Dense1_numNeurons = 128 # Number of Nodes in the First Dense Layer\n",
    "numOutput = 11 # Number of Output Nodes\n",
    "\n",
    "# Weight Initialization\n",
    "weightInit = 'glorot_uniform' # Xavier Initialization\n",
    "\n",
    "# Activation Functions\n",
    "activationHidden = 'relu'\n",
    "activationOutput = 'softmax'\n",
    "\n",
    "# Loss Function\n",
    "lossFunction = 'categorical_crossentropy'\n",
    "\n",
    "# Learning Algorithm\n",
    "netOptimizer = 'adam'\n",
    "\n",
    "# Callbacks\n",
    "callbackList = [\n",
    "        tensorflow.keras.callbacks.ModelCheckpoint('Data/CNN-12/CNN-12-Weights_best.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "        tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')]\n",
    "\n",
    "# Construct Network\n",
    "model = models.Sequential()\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_1'))\n",
    "model.add(ZeroPadding2D(padding=(0,1),name='ZPad1'))\n",
    "model.add(Convolution2D(filters=CNN1_numFilt, kernel_size=CNN1_kernSize, activation=activationHidden, name='Conv_1'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_2'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(ZeroPadding2D(padding=(1,1),input_shape=inpShape,name='ZPad2'))\n",
    "model.add(Convolution2D(filters=CNN2_numFilt, kernel_size=CNN2_kernSize, activation=activationHidden, name='Conv_2'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_3'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Convolution2D(filters=CNN3_numFilt, kernel_size=CNN3_kernSize, activation=activationHidden, name='Conv_3'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_4'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Convolution2D(filters=CNN4_numFilt, kernel_size=CNN4_kernSize, activation=activationHidden, name='Conv_4'))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_5'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(Dense1_numNeurons, activation=activationHidden))\n",
    "model.add(BatchNormalization(axis=-1,input_shape=inpShape, name='Normalization_6'))\n",
    "model.add(Dropout(dropoutRate))\n",
    "model.add(Dense(numOutput, activation=activationOutput))\n",
    "model.compile(loss=lossFunction, optimizer=netOptimizer,metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 100\n",
    "batchSize = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 11000 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[1024,80,3,124] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/Normalization_5/FusedBatchNormV3 (defined at <ipython-input-7-de2e952720ce>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_2908]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-de2e952720ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumEpochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbackList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mc:\\users\\aksha\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[1024,80,3,124] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/Normalization_5/FusedBatchNormV3 (defined at <ipython-input-7-de2e952720ce>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_2908]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=numEpochs,batch_size=batchSize,validation_data=(X_valid,y_valid),callbacks=callbackList,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save History\n",
    "np_loss_history = np.array(history.history[\"loss\"])\n",
    "np.save('Data/CNN-12/lossHist.npy',np_loss_history)\n",
    "\n",
    "np_accu_history = np.array(history.history[\"categorical_accuracy\"])\n",
    "np.save('Data/CNN-12/accuHist.npy',np_accu_history)\n",
    "\n",
    "np_val_loss_history = np.array(history.history[\"val_loss\"])\n",
    "np.save('Data/CNN-12/valLossHist.npy',np_val_loss_history)\n",
    "\n",
    "np_val_accu_history = np.array(history.history[\"val_categorical_accuracy\"])\n",
    "np.save('Data/CNN-12/valAccuHist.npy',np_val_accu_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Evaluate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load Best Weights\n",
    "model.load_weights('Data/CNN-12/CNN-12-Weights_best.h5')\n",
    "\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Details of History\n",
    "lHist = np.load('Data/CNN-12/lossHist.npy')\n",
    "aHist = np.load('Data/CNN-12/accuHist.npy')\n",
    "\n",
    "vLHist = np.load('Data/CNN-12/valLossHist.npy')\n",
    "vAHist = np.load('Data/CNN-12/valAccuHist.npy')\n",
    "\n",
    "# Show loss curves \n",
    "plt.figure()\n",
    "plt.title('CNN 12 - Training performance')\n",
    "plt.plot(lHist, label='Training Loss')\n",
    "plt.plot(vLHist, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0,len(lHist)+1,2),np.arange(1,len(lHist)+1,2))\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract Test Data of Specific SNR\n",
    "def extractTest(data,labels,labelsEncoded,testIndex,snr):\n",
    "    testData = data[testIndex]\n",
    "    labelArray = np.array([labels])\n",
    "    testLabels = labelArray[:,testIdx,:]\n",
    "    testLabelsEncoded = labelsEncoded[testIdx]\n",
    "    \n",
    "    idxOP = list()\n",
    "    \n",
    "    # Loop Through Label Array To Get Index of Specific SNR\n",
    "    for i in range(0,testLabels.shape[1]):\n",
    "        if testLabels[0,i,1].decode('ascii')==snr:\n",
    "            idxOP.append(i)\n",
    "    \n",
    "    # Return Subset of Test Data and Corresponding Labels\n",
    "    opTestData = np.expand_dims(testData[idxOP,:,:],axis=-1)\n",
    "    opTestLabel = testLabelsEncoded[idxOP]\n",
    "    \n",
    "    return opTestData, opTestLabel\n",
    "\n",
    "def plot_confusion_matrix(cm, titleAdd, title='CNN 11 - Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title+titleAdd)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Confusion Matrix Function\n",
    "def prepConfMat(testData,testLabel,predTestLabel,mods,title):\n",
    "    modString = list()\n",
    "    for i in range(0,len(mods)):\n",
    "        modString.append(mods[i].decode('ascii'))\n",
    "    \n",
    "    conf = np.zeros([len(mods),len(mods)])\n",
    "    confnorm = np.zeros([len(mods),len(mods)])\n",
    "    for i in range(0,testData.shape[0]):\n",
    "        j = list(testLabel[i,:]).index(1)\n",
    "        k = int(np.argmax(predTestLabel[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(mods)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    plot_confusion_matrix(confnorm, title, labels=modString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "test_Y_hat = model.predict(X_test, batch_size=1024)\n",
    "\n",
    "prepConfMat(X_test,y_test,test_Y_hat,mods,' (All SNRs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix for Specific SNR\n",
    "snr = '0'\n",
    "title = ' (SNR = '+snr+')'\n",
    "x_testSNR, y_TestSNR = extractTest(X,lbl,lbl_encoded,testIdx,snr)\n",
    "y_hat_snr = model.predict(x_testSNR, batch_size=1024)\n",
    "prepConfMat(x_testSNR,y_TestSNR,y_hat_snr,mods,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test accuracy for different SNRs\n",
    "acc = {}\n",
    "acc_array=[]\n",
    "\n",
    "snr_array=np.asarray(lbl)[:,1]\n",
    "lb_temp = preprocessing.LabelBinarizer()\n",
    "lb_temp.fit(snr_array)\n",
    "temp_array=lb_temp.classes_\n",
    "snr_label_array = []\n",
    "\n",
    "\n",
    "snr_label_array.append(temp_array[6])\n",
    "snr_label_array.append(temp_array[4])\n",
    "snr_label_array.append(temp_array[3])\n",
    "snr_label_array.append(temp_array[2])\n",
    "snr_label_array.append(temp_array[1])\n",
    "snr_label_array.append(temp_array[0])\n",
    "snr_label_array.append(temp_array[9])\n",
    "snr_label_array.append(temp_array[8])\n",
    "snr_label_array.append(temp_array[7])\n",
    "snr_label_array.append(temp_array[5])\n",
    "snr_label_array.append(temp_array[10])\n",
    "snr_label_array.append(temp_array[16])\n",
    "snr_label_array.append(temp_array[17])\n",
    "snr_label_array.append(temp_array[18])\n",
    "snr_label_array.append(temp_array[19])\n",
    "snr_label_array.append(temp_array[11])\n",
    "snr_label_array.append(temp_array[12])\n",
    "snr_label_array.append(temp_array[13])\n",
    "snr_label_array.append(temp_array[14])\n",
    "snr_label_array.append(temp_array[15])\n",
    "\n",
    "\n",
    "#print(snr_label_array)\n",
    "y_test_snr=snr_array[testIdx]\n",
    "\n",
    "\n",
    "\n",
    "for snr in snr_label_array:\n",
    "    test_X_i = X_test[np.where(y_test_snr==snr)]\n",
    "    test_Y_i = y_test[np.where(y_test_snr==snr)]\n",
    "    \n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([len(mods),len(mods)])\n",
    "    confnorm = np.zeros([len(mods),len(mods)])\n",
    "    for i in range(0,test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i,:]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(mods)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print(\"Overall Accuracy: \", cor / (cor+ncor),\"for SNR\",snr)\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "    acc_array.append(1.0*cor/(cor+ncor))\n",
    "\n",
    "print(\"Random Guess Accuracy:\",1/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show loss curves \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('CNN 12 - Accuracy vs SNRs')\n",
    "plt.plot(np.arange(-20,20,2), acc_array)\n",
    "plt.xlabel('SNR')\n",
    "plt.xticks(np.arange(-20,20,2))\n",
    "plt.ylabel('Class Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accRes = np.array([acc_array])\n",
    "np.save('Data/CNN-12/accResSNR.npy',accRes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
